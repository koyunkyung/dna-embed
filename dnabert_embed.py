import torch
import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertModel  
from typing import List, Tuple
import warnings
warnings.filterwarnings('ignore')


class DNABertEmbedder:

    def __init__(self, model_name: str = "zhihan1996/DNA_bert_6", 
                 max_length: int = 512,
                 embedding_dim: int = 768,
                 device: str = None):
        
        self.model_name = model_name
        self.max_length = max_length
        self.embedding_dim = min(embedding_dim, 2048)  # ê²½ì§„ëŒ€íšŒ ì œí•œ
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {self.device}")
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}...")
        
        # BertTokenizerì™€ BertModel ì‚¬ìš©
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertModel.from_pretrained(model_name).to(self.device)
        
        self.model.eval()  
        
        # DNA-BERTì˜ ê¸°ë³¸ ì„ë² ë”© ì°¨ì› í™•ì¸
        self.base_dim = self.model.config.hidden_size
        print(f"ê¸°ë³¸ ì„ë² ë”© ì°¨ì›: {self.base_dim}")
        
        # ì°¨ì› ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš° Linear layer ì¶”ê°€
        if self.embedding_dim != self.base_dim:
            self.projection = torch.nn.Linear(self.base_dim, self.embedding_dim).to(self.device)
            print(f"ì„ë² ë”© ì°¨ì›ì„ {self.base_dim} -> {self.embedding_dim}ìœ¼ë¡œ ì¡°ì •")
        else:
            self.projection = None
        
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
    
    def kmer_tokenize(self, sequence: str, k: int = 6) -> str:
        # ëŒ€ë¬¸ìë¡œ ë³€í™˜
        sequence = sequence.upper()
        
        # k-mer ìƒì„±
        kmers = []
        for i in range(len(sequence) - k + 1):
            kmer = sequence[i:i+k]
            # ìœ íš¨í•œ ì—¼ê¸°(A, C, G, T)ë§Œ í¬í•¨ëœ k-merë§Œ ì¶”ê°€
            if all(base in 'ACGT' for base in kmer):
                kmers.append(kmer)
        
        return ' '.join(kmers)
    
    def encode_sequence(self, sequence: str) -> torch.Tensor:
        """ë‹¨ì¼ DNA ì„œì—´ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        # k-mer í† í°í™”
        kmer_seq = self.kmer_tokenize(sequence)
        
        # í† í°í™”
        inputs = self.tokenizer(
            kmer_seq,
            return_tensors='pt',
            max_length=self.max_length,
            padding='max_length',
            truncation=True
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì„ë² ë”© ì¶”ì¶œ
        with torch.no_grad():
            outputs = self.model(**inputs)
            # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš© (ë¬¸ì¥ ì „ì²´ë¥¼ ëŒ€í‘œ)
            embedding = outputs.last_hidden_state[:, 0, :]  # shape: (1, hidden_size)
        
        # ì°¨ì› ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš°
        if self.projection is not None:
            embedding = self.projection(embedding)
        
        return embedding.squeeze(0)  # shape: (embedding_dim,)
    
    def encode_batch(self, sequences: List[str], batch_size: int = 8) -> np.ndarray:
        
        embeddings = []
        
        print(f"ì´ {len(sequences)}ê°œ ì„œì—´ ì„ë² ë”© ì¤‘...")
        
        for i in range(0, len(sequences), batch_size):
            batch_sequences = sequences[i:i+batch_size]
            
            # ë°°ì¹˜ ë‚´ ê° ì„œì—´ ì²˜ë¦¬
            batch_embeddings = []
            for seq in batch_sequences:
                emb = self.encode_sequence(seq)
                batch_embeddings.append(emb.cpu().numpy())
            
            embeddings.extend(batch_embeddings)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (i + batch_size) % 100 == 0 or (i + batch_size) >= len(sequences):
                print(f"  ì§„í–‰: {min(i + batch_size, len(sequences))}/{len(sequences)}")
        
        embeddings_array = np.array(embeddings)
        print(f"ì„ë² ë”© ì™„ë£Œ! ìµœì¢… shape: {embeddings_array.shape}\n")
        
        return embeddings_array
    
    def save_embeddings(self, embeddings: np.ndarray, ids: List[str], output_path: str):

        # ì»¬ëŸ¼ ì´ë¦„ ìƒì„± (emb_0000, emb_0001, ..., emb_0767 í˜•ì‹)
        n_dims = embeddings.shape[1]
        column_names = [f'emb_{i:04d}' for i in range(n_dims)]
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(embeddings, columns=column_names)
        df.insert(0, 'ID', ids)
        
        # CSVë¡œ ì €ì¥
        df.to_csv(output_path, index=False)
        print(f"ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"íŒŒì¼ í˜•íƒœ: {df.shape}")
        print(f"ì»¬ëŸ¼: ID, emb_0000 ~ emb_{n_dims-1:04d}")


def main():

    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    input_csv = "./data/test.csv"
    output_csv = "./data/output/dnabert_embeddings.csv"
    
    print("="*60)
    print("DNA-BERT ì„ë² ë”© ì¶”ì¶œ ì‹œì‘")
    print("="*60 + "\n")
    
    # 1. ë°ì´í„° ë¡œë“œ
    print(f"ì…ë ¥ íŒŒì¼: {input_csv}")
    df = pd.read_csv(input_csv)
    print(f"ğŸ“ ë¡œë“œëœ ë°ì´í„°: {len(df)}ê°œ ì„œì—´\n")
    
    # 2. DNA-BERT ì„ë² ë” ì´ˆê¸°í™”
    embedder = DNABertEmbedder(
        model_name="zhihan1996/DNA_bert_6",
        max_length=512,
        embedding_dim=768,  # ê¸°ë³¸ 768ì°¨ì› ì‚¬ìš© (í•„ìš”ì‹œ 2048ê¹Œì§€ ê°€ëŠ¥)
        device='cpu'  # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ 'cuda'ë¡œ ë³€ê²½
    )
    
    # 3. ì„ë² ë”© ìƒì„±
    embeddings = embedder.encode_batch(
        sequences=df['seq'].tolist(),
        batch_size=4
    )
    
    # 4. ê²°ê³¼ ì €ì¥
    embedder.save_embeddings(
        embeddings=embeddings,
        ids=df['ID'].tolist(),
        output_path=output_csv
    )
    
    print("\n" + "="*60)
    print("ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ!")
    print("="*60)
    
    return output_csv


if __name__ == "__main__":
    output_file = main()