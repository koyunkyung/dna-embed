import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForMaskedLM
from typing import List
import warnings
warnings.filterwarnings('ignore')


class NucleotideTransformerEmbedder:
    
    def __init__(self, 
                 model_name: str = "InstaDeepAI/nucleotide-transformer-2.5b-multi-species",
                 max_length: int = 1000,
                 embedding_dim: int = 768,
                 device: str = None):
        
        self.model_name = model_name
        self.max_length = max_length
        self.embedding_dim = min(embedding_dim, 2048)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # Nucleotide Transformer ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        self.model = AutoModelForMaskedLM.from_pretrained(
            model_name,
            trust_remote_code=True
        ).to(self.device)
        
        self.model.eval()
        
        # ëª¨ë¸ì˜ ê¸°ë³¸ ì„ë² ë”© ì°¨ì›
        # Nucleotide TransformerëŠ” esm (Evolutionary Scale Modeling) êµ¬ì¡° ì‚¬ìš©
        self.base_dim = self.model.config.hidden_size
        print(f"âœ… ê¸°ë³¸ ì„ë² ë”© ì°¨ì›: {self.base_dim}")
        
        # ì°¨ì› ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš°
        if self.embedding_dim != self.base_dim:
            self.projection = torch.nn.Linear(self.base_dim, self.embedding_dim).to(self.device)
            print(f"ğŸ“ ì„ë² ë”© ì°¨ì› ì¡°ì •: {self.base_dim} -> {self.embedding_dim}")
        else:
            self.projection = None
        
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
    
    def prepare_sequence(self, sequence: str) -> str:
        # ëŒ€ë¬¸ìë¡œ ë³€í™˜
        sequence = sequence.upper()
        
        # ìœ íš¨í•œ ì—¼ê¸°ë§Œ í•„í„°ë§
        valid_bases = set('ACGT')
        filtered_seq = ''.join([base for base in sequence if base in valid_bases])
        
        # ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ (ê° ì—¼ê¸°ë¥¼ ê°œë³„ í† í°ìœ¼ë¡œ)
        spaced_seq = ' '.join(filtered_seq)
        
        return spaced_seq
    
    def encode_sequence(self, sequence: str, pooling: str = 'mean') -> torch.Tensor:
    
        # ì„œì—´ ì „ì²˜ë¦¬
        prepared_seq = self.prepare_sequence(sequence)
        
        # í† í°í™”
        inputs = self.tokenizer(
            prepared_seq,
            return_tensors='pt',
            max_length=self.max_length,
            padding='max_length',
            truncation=True
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì„ë² ë”© ì¶”ì¶œ
        with torch.no_grad():
            # Nucleotide Transformerì˜ ì¶œë ¥ êµ¬ì¡°
            outputs = self.model.esm(**inputs, output_hidden_states=True)
            hidden_states = outputs.hidden_states[-1]  # ë§ˆì§€ë§‰ ë ˆì´ì–´
            
            # Pooling ì „ëµ ì„ íƒ
            if pooling == 'cls':
                # [CLS] í† í° (ì²« ë²ˆì§¸ í† í°)
                embedding = hidden_states[:, 0, :]
            elif pooling == 'mean':
                # í‰ê·  í’€ë§ (attention mask ê³ ë ¤)
                attention_mask = inputs['attention_mask']
                mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
                sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)
                sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
                embedding = sum_embeddings / sum_mask
            elif pooling == 'max':
                # ìµœëŒ€ í’€ë§
                embedding = torch.max(hidden_states, dim=1)[0]
            else:
                # ê¸°ë³¸: mean pooling
                attention_mask = inputs['attention_mask']
                mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
                sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)
                sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
                embedding = sum_embeddings / sum_mask
        
        # ì°¨ì› ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš°
        if self.projection is not None:
            embedding = self.projection(embedding)
        
        return embedding.squeeze(0)
    
    def encode_batch(self, sequences: List[str], batch_size: int = 4, 
                     pooling: str = 'mean') -> np.ndarray:
        
        embeddings = []
        total = len(sequences)
        
        print(f"ğŸ§¬ {total}ê°œ ì„œì—´ ì„ë² ë”© ì¤‘ (pooling: {pooling})...")
        
        for i in range(0, total, batch_size):
            batch_sequences = sequences[i:i+batch_size]
            batch_embeddings = []
            
            for seq in batch_sequences:
                emb = self.encode_sequence(seq, pooling=pooling)
                batch_embeddings.append(emb.cpu().detach().numpy())
            
            embeddings.extend(batch_embeddings)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (i + batch_size) % 50 == 0 or (i + batch_size) >= total:
                progress = min(i + batch_size, total)
                print(f"  â³ ì§„í–‰: {progress}/{total} ({progress/total*100:.1f}%)")
        
        embeddings_array = np.array(embeddings)
        print(f"âœ… ì„ë² ë”© ì™„ë£Œ! shape: {embeddings_array.shape}\n")
        
        return embeddings_array
    
    def save_embeddings(self, embeddings: np.ndarray, ids: List[str], output_path: str):
        
        # ì»¬ëŸ¼ ì´ë¦„ ìƒì„± (emb_0000, emb_0001, ..., emb_0767 í˜•ì‹)
        n_dims = embeddings.shape[1]
        column_names = [f'emb_{i:04d}' for i in range(n_dims)]
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(embeddings, columns=column_names)
        df.insert(0, 'ID', ids)
        
        # CSVë¡œ ì €ì¥
        df.to_csv(output_path, index=False)
        print(f"ğŸ’¾ ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   - íŒŒì¼ í˜•íƒœ: {df.shape}")
        print(f"   - ì»¬ëŸ¼: ID, emb_0000 ~ emb_{n_dims-1:04d}")


def main():
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    input_csv = "./data/test.csv"
    output_csv = "./data/output/nucleotide_embeddings.csv"
    
    df = pd.read_csv(input_csv)
    print(f"âœ… ë¡œë“œëœ ë°ì´í„°: {len(df)}ê°œ ì„œì—´\n")
    
    # 2. Nucleotide Transformer ì´ˆê¸°í™”
    embedder = NucleotideTransformerEmbedder(
        model_name="InstaDeepAI/nucleotide-transformer-2.5b-multi-species",
        max_length=1000,  # Nucleotide TransformerëŠ” ë” ê¸´ ì„œì—´ ì²˜ë¦¬ ê°€ëŠ¥
        embedding_dim=768,  # ê²½ì§„ëŒ€íšŒ í˜•ì‹ (768ì°¨ì›)
        device='cpu'  
    )
    
    # 3. ì„ë² ë”© ìƒì„±
    embeddings = embedder.encode_batch(
        sequences=df['seq'].tolist(),
        batch_size=2,  # í° ëª¨ë¸ì´ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì‘ê²Œ
        pooling='mean'  # mean poolingì´ ë³€ì´ ê°ì§€ì— ë” ì¢‹ìŒ
    )
    
    # 4. ê²°ê³¼ ì €ì¥
    embedder.save_embeddings(
        embeddings=embeddings,
        ids=df['ID'].tolist(),
        output_path=output_csv
    )
    
    print(f"   - ì…ë ¥ ì„œì—´: {len(df)}ê°œ")
    print(f"   - ì„ë² ë”© ì°¨ì›: {embeddings.shape[1]}")
    print(f"   - ì¶œë ¥ íŒŒì¼: {output_csv}")
    
    return output_csv


if __name__ == "__main__":
    output_file = main()