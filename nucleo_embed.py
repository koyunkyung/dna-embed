import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForMaskedLM
from typing import List
import warnings
import time
from concurrent.futures import ThreadPoolExecutor
warnings.filterwarnings('ignore')


class NucleotideTransformerEmbedder:
    
    def __init__(self, 
                 model_name: str = "InstaDeepAI/nucleotide-transformer-2.5b-multi-species",
                 max_length: int = 1000,
                 embedding_dim: int = None,
                 device: str = 'cuda:0',
                 use_fp16: bool = True):
        
        self.model_name = model_name
        self.max_length = max_length
        self.use_fp16 = use_fp16
        self.device = torch.device(device)
        
        print(f"ğŸš€ ë””ë°”ì´ìŠ¤: {self.device}")
        if self.device.type == 'cuda':
            gpu_id = self.device.index if self.device.index is not None else 0
            print(f"   GPU: {torch.cuda.get_device_name(gpu_id)} "
                  f"({torch.cuda.get_device_properties(gpu_id).total_memory / 1e9:.1f} GB)")
        
        # Nucleotide Transformer ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        # FP16 ì‚¬ìš© ì‹œ
        if use_fp16 and self.device.type == 'cuda':
            self.model = AutoModelForMaskedLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16
            ).to(self.device)
            self.dtype = torch.float16
        else:
            self.model = AutoModelForMaskedLM.from_pretrained(
                model_name,
                trust_remote_code=True
            ).to(self.device)
            self.dtype = torch.float32
        
        self.model.eval()
        
        # ëª¨ë¸ì˜ ê¸°ë³¸ ì„ë² ë”© ì°¨ì›
        self.base_dim = self.model.config.hidden_size
        
        # embedding_dimì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì°¨ì› ì‚¬ìš©
        if embedding_dim is None:
            self.embedding_dim = self.base_dim
        else:
            self.embedding_dim = min(embedding_dim, 2048)
            
        # ì°¨ì› ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ projection ìƒì„±
        if self.embedding_dim != self.base_dim:
            self.projection = torch.nn.Linear(self.base_dim, self.embedding_dim)
            self.projection = self.projection.to(dtype=self.dtype, device=self.device)
        else:
            self.projection = None
    
    def prepare_sequence(self, sequence: str) -> str:
        sequence = sequence.upper()
        valid_bases = set('ACGT')
        filtered_seq = ''.join([base for base in sequence if base in valid_bases])
        spaced_seq = ' '.join(filtered_seq)
        return spaced_seq
    
    def encode_batch(self, sequences: List[str], batch_size: int = 16, 
                     pooling: str = 'mean') -> np.ndarray:
        """
        ë‹¨ì¼ GPUì—ì„œ ë°°ì¹˜ ì²˜ë¦¬
        """
        embeddings = []
        total = len(sequences)
        
        for i in range(0, total, batch_size):
            batch_sequences = sequences[i:i+batch_size]
            
            # ë°°ì¹˜ ì „ì²˜ë¦¬
            prepared_seqs = [self.prepare_sequence(seq) for seq in batch_sequences]
            
            # ë°°ì¹˜ í† í°í™”
            inputs = self.tokenizer(
                prepared_seqs,
                return_tensors='pt',
                max_length=self.max_length,
                padding='max_length',
                truncation=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ë°°ì¹˜ ì„ë² ë”© ì¶”ì¶œ
            with torch.no_grad():
                outputs = self.model.esm(**inputs, output_hidden_states=True)
                hidden_states = outputs.hidden_states[-1]
                
                # Mean pooling
                attention_mask = inputs['attention_mask']
                mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).to(hidden_states.dtype)
                sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)
                sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
                batch_embeddings = sum_embeddings / sum_mask
                
                # ì°¨ì› ì¡°ì •
                if self.projection is not None:
                    batch_embeddings = self.projection(batch_embeddings)
                batch_embeddings = torch.nan_to_num(
                    batch_embeddings,
                    nan=0.0,
                    posinf=1e4,
                    neginf=-1e4,
                )
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            embeddings.append(batch_embeddings.cpu().float().detach().numpy())
        
        return np.vstack(embeddings)


class MultiGPUEmbedder:
    
    def __init__(self, 
                 model_name: str = "InstaDeepAI/nucleotide-transformer-2.5b-multi-species",
                 max_length: int = 1000,
                 embedding_dim: int = None,
                 use_fp16: bool = True):
        
        self.n_gpus = torch.cuda.device_count()
        print(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {self.n_gpus}ê°œ")
        
        for i in range(self.n_gpus):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)} "
                  f"({torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB)")
        print()
        
        # ê° GPUì— ë³„ë„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        print("ğŸ“¥ GPUë³„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embedders = []
        
        for gpu_id in range(min(self.n_gpus, 2)):  # ìµœëŒ€ 2ê°œ GPU ì‚¬ìš©
            print(f"\nğŸ”§ GPU {gpu_id} ì´ˆê¸°í™” ì¤‘...")
            embedder = NucleotideTransformerEmbedder(
                model_name=model_name,
                max_length=max_length,
                embedding_dim=embedding_dim,
                device=f'cuda:{gpu_id}',
                use_fp16=use_fp16
            )
            self.embedders.append(embedder)
        
        self.n_workers = len(self.embedders)
        print(f"\nâœ… {self.n_workers}ê°œ GPU ì¤€ë¹„ ì™„ë£Œ!\n")
    
    def encode_batch_multi_gpu(self, sequences: List[str], batch_size: int = 16,
                              pooling: str = 'mean') -> np.ndarray:
        """
        ì—¬ëŸ¬ GPUì—ì„œ ë³‘ë ¬ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        """
        total = len(sequences)
        
        # ë°ì´í„°ë¥¼ GPU ê°œìˆ˜ë§Œí¼ ë¶„í• 
        chunk_size = (total + self.n_workers - 1) // self.n_workers
        chunks = [sequences[i:i+chunk_size] for i in range(0, total, chunk_size)]
        
        print(f"ğŸ§¬ {total}ê°œ ì„œì—´ ì„ë² ë”© ì¤‘")
        print(f"   - GPU ê°œìˆ˜: {self.n_workers}ê°œ")
        print(f"   - GPUë‹¹ ì²˜ë¦¬: {chunk_size}ê°œ")
        print(f"   - Batch size: {batch_size}")
        print(f"   - Pooling: {pooling}")
        print()
        
        start_time = time.time()
        
        def process_chunk(gpu_id, chunk):
            """ê° GPUì—ì„œ chunk ì²˜ë¦¬"""
            embedder = self.embedders[gpu_id]
            chunk_embeddings = []
            
            for i in range(0, len(chunk), batch_size):
                batch_start = time.time()
                batch = chunk[i:i+batch_size]
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_emb = embedder.encode_batch(batch, batch_size=len(batch), pooling=pooling)
                chunk_embeddings.append(batch_emb)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                progress = i + len(batch)
                if progress % 100 == 0 or progress >= len(chunk):
                    batch_time = time.time() - batch_start
                    mem_allocated = torch.cuda.memory_allocated(gpu_id) / 1e9
                    mem_reserved = torch.cuda.memory_reserved(gpu_id) / 1e9
                    
                    print(f"  GPU{gpu_id} â³ {progress}/{len(chunk)} ({progress/len(chunk)*100:.1f}%) | "
                          f"ë°°ì¹˜: {batch_time:.2f}s | "
                          f"ë©”ëª¨ë¦¬: {mem_allocated:.1f}/{mem_reserved:.1f}GB")
            
            return np.vstack(chunk_embeddings)
        
        # ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ê° GPUì—ì„œ ë™ì‹œ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:
            futures = [executor.submit(process_chunk, gpu_id, chunk) 
                      for gpu_id, chunk in enumerate(chunks)]
            results = [future.result() for future in futures]
        
        # ê²°ê³¼ í•©ì¹˜ê¸°
        embeddings_array = np.vstack(results)
        total_time = time.time() - start_time
        
        print(f"\nâœ… ì„ë² ë”© ì™„ë£Œ! shape: {embeddings_array.shape}")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time/60:.2f}ë¶„ ({total_time:.1f}ì´ˆ)")
        print(f"ğŸ“Š í‰ê·  ì†ë„: {total/total_time:.1f} sequences/second")
        print(f"ğŸš€ GPUë‹¹ ì²˜ë¦¬ëŸ‰: {total/total_time/self.n_workers:.1f} sequences/second/GPU\n")
        
        return embeddings_array
    
    def save_embeddings(self, embeddings: np.ndarray, ids: List[str], output_path: str):
        # ì»¬ëŸ¼ ì´ë¦„ ìƒì„±
        n_dims = embeddings.shape[1]
        column_names = [f'emb_{i:04d}' for i in range(n_dims)]
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(embeddings, columns=column_names)
        df.insert(0, 'ID', ids)
        
        # CSVë¡œ ì €ì¥
        print(f"ğŸ’¾ CSV ì €ì¥ ì¤‘...")
        df.to_csv(output_path, index=False)
        print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   - íŒŒì¼ í˜•íƒœ: {df.shape}")
        print(f"   - ì»¬ëŸ¼: ID, emb_0000 ~ emb_{n_dims-1:04d}")


def main():
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    input_csv = "./data/test.csv"
    output_csv = "./data/output/nucleotide_embeddings.csv"
    
    print("="*60)
    print("ğŸ§¬ Nucleotide Transformer ì„ë² ë”© ìƒì„± (Multi-GPU)")
    print("="*60)
    print()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë°ì´í„° ë¡œë”©...")
    df = pd.read_csv(input_csv)
    print(f"âœ… ë¡œë“œëœ ë°ì´í„°: {len(df):,}ê°œ ì„œì—´\n")
    
    # 2. Multi-GPU Embedder ì´ˆê¸°í™”
    embedder = MultiGPUEmbedder(
        model_name="InstaDeepAI/nucleotide-transformer-2.5b-multi-species",
        max_length=1000,
        embedding_dim=2048,  # 2048ì°¨ì›
        use_fp16=True
    )
    
    # 3. ì„ë² ë”© ìƒì„± (ë©€í‹° GPU)
    embeddings = embedder.encode_batch_multi_gpu(
        sequences=df['seq'].tolist(),
        batch_size=16,  # ê° GPUë‹¹ ë°°ì¹˜ í¬ê¸°
        pooling='mean'
    )
    
    # 4. ê²°ê³¼ ì €ì¥
    embedder.save_embeddings(
        embeddings=embeddings,
        ids=df['ID'].tolist(),
        output_path=output_csv
    )
    
    return output_csv


if __name__ == "__main__":
    output_file = main()