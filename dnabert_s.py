import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel # ë³€ê²½ë¨
from typing import List
import warnings
warnings.filterwarnings('ignore')

class DNABertSEmbedder:

    def __init__(self, model_name: str = "zhihan1996/DNABERT-S", 
                 max_length: int = 512,
                 embedding_dim: int = 768,
                 device: str = None):
        
        self.model_name = model_name
        self.max_length = max_length
        self.embedding_dim = min(embedding_dim, 2048)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {self.device}")
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}...")
        gpu_count = torch.cuda.device_count()
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {gpu_count}ê°œ")
        
        # [ë³€ê²½] DNABERT-SëŠ” AutoClassì™€ trust_remote_code=Trueê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        base_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        
        
        # ê¸°ë³¸ ì„ë² ë”© ì°¨ì› í™•ì¸ (ì¼ë°˜ì ìœ¼ë¡œ 768)
        # DNABERT-S ëª¨ë¸ config êµ¬ì¡°ì— ë”°ë¼ hidden_size ì†ì„± ìœ„ì¹˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        if hasattr(self.model.config, 'hidden_size'):
            self.base_dim = self.model.config.hidden_size
        else:
            self.base_dim = 768 # Default for BERT base
            
        print(f"ê¸°ë³¸ ì„ë² ë”© ì°¨ì›: {self.base_dim}")

        base_model.to(self.device)

        if gpu_count > 1:
            print(f"ğŸ”¥ {gpu_count}ê°œì˜ GPU(A40)ë¥¼ ë³‘ë ¬ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤!")
            self.model = torch.nn.DataParallel(base_model)
        else:
            self.model = base_model

        self.model.eval()
        
        # ì°¨ì› ì¡°ì •
        self.embedding_dim = min(embedding_dim, 2048)
        if self.embedding_dim != self.base_dim:
            self.projection = torch.nn.Linear(self.base_dim, self.embedding_dim).to(self.device)
        else:
            self.projection = None
            
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
    
    # [ì‚­ì œ] DNABERT-SëŠ” k-mer í† í°í™” í•¨ìˆ˜ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    
    def encode_sequence(self, sequence: str) -> torch.Tensor:
        """ë‹¨ì¼ DNA ì„œì—´ì„ DNABERT-S ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        
        # DNA ì„œì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        dna_seq = sequence.upper() 
        
        # í† í°í™” (Raw String ì…ë ¥)
        inputs = self.tokenizer(
            dna_seq,
            return_tensors='pt',
            max_length=self.max_length,
            padding='max_length',
            truncation=True
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì„ë² ë”© ì¶”ì¶œ
        with torch.no_grad():
            outputs = self.model(inputs["input_ids"])
            hidden_states = outputs[0] # [1, sequence_length, 768]
            
            # [ë³€ê²½] Mean Pooling ì‚¬ìš© (DNABERT-S ê¶Œì¥ ë°©ì‹)
            # Padding ë¶€ë¶„ì€ ì œì™¸í•˜ê³  í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ ì •ì„ì´ì§€ë§Œ, 
            # ê°„ë‹¨í•œ êµ¬í˜„ì„ ìœ„í•´ ì „ì²´ í‰ê· ì„ ì‚¬ìš©í•˜ê±°ë‚˜(ë…¼ë¬¸ êµ¬í˜„ì²´ ë°©ì‹), attention maskë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” DNABERT-S ê³µì‹ ì˜ˆì œ ì½”ë“œì¸ torch.mean(hidden_states[0], dim=0) ë°©ì‹ì„ ë”°ë¦…ë‹ˆë‹¤.
            embedding = torch.mean(hidden_states[0], dim=0) # shape: (hidden_size,)
        
        # ì°¨ì› ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš°
        if self.projection is not None:
            embedding = self.projection(embedding)
        
        return embedding # shape: (embedding_dim,)
    
    def encode_batch(self, sequences: List[str], batch_size: int = 8) -> np.ndarray:
        
        embeddings = []
        
        print(f"ì´ {len(sequences)}ê°œ ì„œì—´ ì„ë² ë”© ì¤‘...")
        
        for i in range(0, len(sequences), batch_size):
            batch_sequences = sequences[i:i+batch_size]
            
            # ë°°ì¹˜ ë‚´ ê° ì„œì—´ ì²˜ë¦¬
            batch_embeddings = []
            for seq in batch_sequences:
                emb = self.encode_sequence(seq)
                batch_embeddings.append(emb.cpu().numpy())
            
            embeddings.extend(batch_embeddings)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (i + batch_size) % 100 == 0 or (i + batch_size) >= len(sequences):
                print(f"  ì§„í–‰: {min(i + batch_size, len(sequences))}/{len(sequences)}")
        
        embeddings_array = np.array(embeddings)
        print(f"ì„ë² ë”© ì™„ë£Œ! ìµœì¢… shape: {embeddings_array.shape}\n")
        
        return embeddings_array
    
    def save_embeddings(self, embeddings: np.ndarray, ids: List[str], output_path: str):

        # ì»¬ëŸ¼ ì´ë¦„ ìƒì„± (emb_0000, emb_0001, ..., emb_0767 í˜•ì‹)
        n_dims = embeddings.shape[1]
        column_names = [f'emb_{i:04d}' for i in range(n_dims)]
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(embeddings, columns=column_names)
        
        # ID ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€, ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•˜ì§€ë§Œ ì—¬ê¸°ì„  ì…ë ¥ë°›ì€ ID ì‚¬ìš©
        if ids is not None and len(ids) == len(df):
            df.insert(0, 'ID', ids)
        
        # CSVë¡œ ì €ì¥
        df.to_csv(output_path, index=False)
        print(f"ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"íŒŒì¼ í˜•íƒœ: {df.shape}")
        if ids is not None:
            print(f"ì»¬ëŸ¼: ID, emb_0000 ~ emb_{n_dims-1:04d}")
        else:
            print(f"ì»¬ëŸ¼: emb_0000 ~ emb_{n_dims-1:04d}")


def main():

    # [ì„¤ì •] íŒŒì¼ ê²½ë¡œ
    input_csv = "./data/test.csv"
    output_csv = "./data/output/dnabert_embeddings.csv"
    
    print("="*60)
    print("DNABERT-S ì„ë² ë”© ì¶”ì¶œ ì‹œì‘")
    print("="*60 + "\n")
    
    # 1. ë°ì´í„° ë¡œë“œ
    # CSV íŒŒì¼ì— 'ID'ì™€ 'seq' ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
    try:
        print(f"ì…ë ¥ íŒŒì¼: {input_csv}")
        df = pd.read_csv(input_csv)
        print(f"ğŸ“ ë¡œë“œëœ ë°ì´í„°: {len(df)}ê°œ ì„œì—´\n")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({input_csv})")
        return

    # 2. DNABERT-S ì„ë² ë” ì´ˆê¸°í™”
    embedder = DNABertSEmbedder(
        model_name="zhihan1996/DNABERT-S", # ëª¨ë¸ëª… ë³€ê²½
        max_length=512,     # í•„ìš”ì— ë”°ë¼ ì¡°ì ˆ (ë„ˆë¬´ ê¸¸ë©´ OOM ë°œìƒ ê°€ëŠ¥)
        embedding_dim=768,  # DNABERT-S ê¸°ë³¸ ì¶œë ¥
        device='cuda'        # GPU ì‚¬ìš© ì‹œ 'cuda'
    )
    
    # 3. ì„ë² ë”© ìƒì„±
    # ë°ì´í„° í”„ë ˆì„ ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ê²½ìš° ìˆ˜ì • í•„ìš” (ì˜ˆ: df['sequence'])
    embeddings = embedder.encode_batch(
        sequences=df['seq'].tolist(),
        batch_size=256 # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ
    )
    
    # 4. ê²°ê³¼ ì €ì¥
    embedder.save_embeddings(
        embeddings=embeddings,
        ids=df['ID'].tolist() if 'ID' in df.columns else None,
        output_path=output_csv
    )
    
    print("\n" + "="*60)
    print("ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ!")
    print("="*60)
    
    return output_csv


if __name__ == "__main__":
    main()